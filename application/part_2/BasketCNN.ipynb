{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Basket Completion\n",
    "\n",
    "The recommendation task of basket completion is a key part of many online retail applications. Basket completion involves computing predictions for the next item that should be added to a shopping basket, given a collection of items that the user has already added to the basket.\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "Amazon Baby Registries - This is a public dataset that consists of registries of baby products\n",
    "from 15 different categories (such as ’feeding’, ’diapers’,’toys’, etc.), where the item catalog and registries for each category are disjoint. Each category therefore provides a small dataset, with a maximum of 15,000 purchased baskets per category. \n",
    "\n",
    "\n",
    "\n",
    "# Solution:\n",
    "\n",
    "1Dimensional CNNs - Similar to what we presented in the TextCNN section of our course.\n",
    "\n",
    "![Foo](./images/textCNN.png)\n",
    "\n",
    "\n",
    "\n",
    "# Metrics:\n",
    "\n",
    "### Precision@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We are going to load the data from a provided URL, build the product vocab, ...\n",
    "\n",
    "But first, let's import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List, Tuple\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a few useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_basket_data_from_url(url: str) -> List[List[int]]:\n",
    "    # We read the data from the provided url\n",
    "    # Each product is represented by a number from 1 to nb_products, so we substract 1 to be zero indexed\n",
    "    dataset = []\n",
    "    with urllib.request.urlopen(url) as data:\n",
    "        for line in data:\n",
    "            line = line.decode(\"utf-8\")\n",
    "            products = [int(p)-1 for p in line.split(',')]\n",
    "            if len(products) > 1:\n",
    "                dataset.append(products)\n",
    "    return dataset\n",
    "\n",
    "def build_vocab(dataset: List[List[int]]) -> Tuple[List[int], Counter]:\n",
    "    # Just counting how many times each products appears and building the list of unique products\n",
    "    counter = Counter()\n",
    "    for basket in dataset:\n",
    "        counter.update(basket)\n",
    "    return list(counter.keys()), counter\n",
    "\n",
    "def to_size(data: List[int], size: int):\n",
    "    # Here we are going to make the baskets of a predetermined size\n",
    "    # either by truncating them, or by duplicating some products\n",
    "    if len(data) > size:\n",
    "        return np.random.choice(data, size=size, replace=False)\n",
    "    else:\n",
    "        return np.random.choice(data, size=size, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now defined our main dataset class `BasketData`.\n",
    "It contains several fields:\n",
    "* dataset: X, y tuple where X is np.array with one basket per row, y is a np.array with the target item to predict\n",
    "* vocab: list of unique products\n",
    "* counter: Counter of unique items and their counts\n",
    "* vocab_size: nb unique products\n",
    "* max_basket_length: maximum size of a seen basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketData(NamedTuple):\n",
    "    dataset: Tuple[np.ndarray, np.ndarray]\n",
    "    vocab: List[int]\n",
    "    counter: Counter\n",
    "    vocab_size: int\n",
    "    max_basket_length: int\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_from_url(url: str):\n",
    "        dataset = read_basket_data_from_url(url)\n",
    "        print(f\"Read {len(dataset)} baskets from {url}\")\n",
    "        \n",
    "        vocab, counter = build_vocab(dataset)\n",
    "        print(f\"Number of distinct products {len(vocab)}\")\n",
    "        \n",
    "        max_basket_length = max(len(b) for b in dataset)\n",
    "        print(f\"Max basket size {max_basket_length}\")\n",
    "        \n",
    "        dataset = BasketData.build_input_and_labels(dataset, max_basket_length)\n",
    "        \n",
    "        print(f\"Done building dataset\")\n",
    "        return BasketData(dataset, vocab, counter, len(vocab), max_basket_length)\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_input_and_labels(baskets: List[List[int]], max_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        for basket in baskets:\n",
    "            input_basket = basket[:-1]\n",
    "            label_product = basket[-1]\n",
    "\n",
    "            inputs.append(to_size(input_basket, max_length))\n",
    "            labels.append(label_product)\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        labels = np.array(labels)\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://www.dropbox.com/s/hkwnwlut4mb5yyb/1_100_100_100_apparel_regs.csv?dl=1\"\n",
    "basket_data = BasketData.build_from_url(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to define the BasketCNN model using Keras layers, the most important operations is the 1D Convolution, using different kernel sizes and max pooling, we will be changing these parameters in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BasketCNN(max_sequence_length, vocab_size, embedding_dim=100, num_filters=16, dropout_rate=0.25):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - max_sequence_length: maximum length of baskets\n",
    "        - vocab_size: number of distinct products\n",
    "        - embedding_layer: embedding layer of Keras created by model type and static flags\n",
    "        - dropout_rate: dropout rate for flattened pooled outputs\n",
    "    Returns:\n",
    "        - model: Model class created with specified inputs\n",
    "    \"\"\"        \n",
    "    x_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                                output_dim=embedding_dim)\n",
    "\n",
    "    x = embedding_layer(x_input)\n",
    "\n",
    "    kernel_sizes = [3, 5, 7]\n",
    "    pooled = []\n",
    "\n",
    "    for kernel in kernel_sizes:\n",
    "\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                      kernel_size=kernel,\n",
    "                      padding='valid',\n",
    "                      strides=1,\n",
    "                      activation='relu')(x)\n",
    "        \n",
    "        pool = MaxPooling1D(pool_size=max_sequence_length - kernel + 1)(conv)\n",
    "\n",
    "        pooled.append(pool)\n",
    "\n",
    "    merged = Concatenate(axis=-1)(pooled)\n",
    "\n",
    "    flatten = Flatten()(merged)\n",
    "\n",
    "    drop = Dropout(rate=dropout_rate)(flatten)\n",
    "    \n",
    "    x_output = Dense(vocab_size, activation='softmax')(drop)\n",
    "\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasketCNN(basket_data.max_basket_length, basket_data.vocab_size, dropout_rate=0.25)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our data to train, test and train our model !\n",
    "We will be using `sparse_categorical_crossentropy` as our loss  and mesuring precision@1 and precision@5 (default k value of `sparse_top_k_categorical_accuracy`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = basket_data.dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance of this model to a very naive baseline that just always predicts the most popular product (the goal is obviously to beat it ^^ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_product, max_count = basket_data.counter.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_labels = np.zeros_like(y_test, dtype=np.float32)\n",
    "naive_labels = most_common_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(naive_labels == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Q1 : What's the impact of the number of filters, kernels sizes, embedding size on the metrics\n",
    "* Q2 : Add a second convolutional layer to the model, how does it impact the model ? Does it overfit ?\n",
    "* Q3 : Try augmenting the dataset with shuffled versions of the baskets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
