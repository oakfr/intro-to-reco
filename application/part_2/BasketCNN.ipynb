{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Basket Completion\n",
    "\n",
    "The recommendation task of basket completion is a key part of many online retail applications. Basket completion involves computing predictions for the next item that should be added to a shopping basket, given a collection of items that the user has already added to the basket.\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "Amazon Baby Registries - This is a public dataset that consists of registries of baby products\n",
    "from 15 different categories (such as ’feeding’, ’diapers’,’toys’, etc.), where the item catalog and registries for each category are disjoint. Each category therefore provides a small dataset, with a maximum of 15,000 purchased baskets per category. \n",
    "\n",
    "\n",
    "\n",
    "# Solution:\n",
    "\n",
    "1Dimensional CNNs - Similar to what we presented in the TextCNN section of our course.\n",
    "\n",
    "![Foo](./images/textCNN.png)\n",
    "\n",
    "\n",
    "\n",
    "# Metrics:\n",
    "\n",
    "### Precision@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We are going to load the data from a provided URL, build the product vocab, ...\n",
    "\n",
    "But first, let's import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List, Tuple\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a few useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_basket_data_from_url(url: str) -> List[List[int]]:\n",
    "    # We read the data from the provided url\n",
    "    # Each product is represented by a number from 1 to nb_products, so we substract 1 to be zero indexed\n",
    "    dataset = []\n",
    "    with urllib.request.urlopen(url) as data:\n",
    "        for line in data:\n",
    "            line = line.decode(\"utf-8\")\n",
    "            products = [int(p)-1 for p in line.split(',')]\n",
    "            if len(products) > 1:\n",
    "                dataset.append(products)\n",
    "    return dataset\n",
    "\n",
    "def build_vocab(dataset: List[List[int]]) -> Tuple[List[int], Counter]:\n",
    "    # Just counting how many times each products appears and building the list of unique products\n",
    "    counter = Counter()\n",
    "    for basket in dataset:\n",
    "        counter.update(basket)\n",
    "    return list(counter.keys()), counter\n",
    "\n",
    "def to_size(data: List[int], size: int):\n",
    "    # Here we are going to make the baskets of a predetermined size\n",
    "    # either by truncating them, or by duplicating some products\n",
    "    if len(data) > size:\n",
    "        return np.random.choice(data, size=size, replace=False)\n",
    "    else:\n",
    "        return np.random.choice(data, size=size, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now defined our main dataset class `BasketData`.\n",
    "It contains several fields:\n",
    "* dataset: X, y tuple where X is np.array with one basket per row, y is a np.array with the target item to predict\n",
    "* vocab: list of unique products\n",
    "* counter: Counter of unique items and their counts\n",
    "* vocab_size: nb unique products\n",
    "* max_basket_length: maximum size of a seen basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketData(NamedTuple):\n",
    "    dataset: Tuple[np.ndarray, np.ndarray]\n",
    "    vocab: List[int]\n",
    "    counter: Counter\n",
    "    vocab_size: int\n",
    "    max_basket_length: int\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_from_url(url: str):\n",
    "        dataset = read_basket_data_from_url(url)\n",
    "        print(f\"Read {len(dataset)} baskets from {url}\")\n",
    "        \n",
    "        vocab, counter = build_vocab(dataset)\n",
    "        print(f\"Number of distinct products {len(vocab)}\")\n",
    "        \n",
    "        max_basket_length = max(len(b) for b in dataset)\n",
    "        print(f\"Max basket size {max_basket_length}\")\n",
    "        \n",
    "        dataset = BasketData.build_input_and_labels(dataset, max_basket_length)\n",
    "        \n",
    "        print(f\"Done building dataset\")\n",
    "        return BasketData(dataset, vocab, counter, len(vocab), max_basket_length)\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_input_and_labels(baskets: List[List[int]], max_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        for basket in baskets:\n",
    "            input_basket = basket[:-1]\n",
    "            label_product = basket[-1]\n",
    "\n",
    "            inputs.append(to_size(input_basket, max_length))\n",
    "            labels.append(label_product)\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        labels = np.array(labels)\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8102 baskets from https://www.dropbox.com/s/hkwnwlut4mb5yyb/1_100_100_100_apparel_regs.csv?dl=1\n",
      "Number of distinct products 100\n",
      "Max basket size 21\n",
      "Done building dataset\n"
     ]
    }
   ],
   "source": [
    "data_url = \"https://www.dropbox.com/s/hkwnwlut4mb5yyb/1_100_100_100_apparel_regs.csv?dl=1\"\n",
    "basket_data = BasketData.build_from_url(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to define the BasketCNN model using Keras layers, the most important operations is the 1D Convolution, using different kernel sizes and max pooling, we will be changing these parameters in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BasketCNN(max_sequence_length, vocab_size, embedding_dim=100, num_filters=16, dropout_rate=0.25):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - max_sequence_length: maximum length of baskets\n",
    "        - vocab_size: number of distinct products\n",
    "        - embedding_layer: embedding layer of Keras created by model type and static flags\n",
    "        - dropout_rate: dropout rate for flattened pooled outputs\n",
    "    Returns:\n",
    "        - model: Model class created with specified inputs\n",
    "    \"\"\"        \n",
    "    x_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                                output_dim=embedding_dim,\n",
    "                                embeddings_initializer='uniform')\n",
    "\n",
    "    x = embedding_layer(x_input)\n",
    "\n",
    "    kernel_sizes = [3, 5, 7]\n",
    "    pooled = []\n",
    "\n",
    "    for kernel in kernel_sizes:\n",
    "\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                      kernel_size=kernel,\n",
    "                      padding='valid',\n",
    "                      strides=1,\n",
    "                      kernel_initializer='he_uniform',\n",
    "                      activation='relu')(x)\n",
    "        \n",
    "        pool = MaxPooling1D(pool_size=max_sequence_length - kernel + 1)(conv)\n",
    "\n",
    "        pooled.append(pool)\n",
    "\n",
    "    merged = Concatenate(axis=-1)(pooled)\n",
    "\n",
    "    flatten = Flatten()(merged)\n",
    "\n",
    "    drop = Dropout(rate=dropout_rate)(flatten)\n",
    "    \n",
    "    x_output = Dense(vocab_size, kernel_initializer='he_uniform', activation='softmax')(drop)\n",
    "\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ma.benhalloum/anaconda/envs/reco/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/ma.benhalloum/anaconda/envs/reco/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 21, 100)      10000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 19, 16)       4816        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 17, 16)       8016        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 16)       11216       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 16)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 16)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 16)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 48)        0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 48)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 48)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          4900        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 38,948\n",
      "Trainable params: 38,948\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BasketCNN(basket_data.max_basket_length, basket_data.vocab_size, dropout_rate=0.25)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our data to train, test and train our model !\n",
    "We will be using `sparse_categorical_crossentropy` as our loss  and mesuring precision@1 and precision@5 (default k value of `sparse_top_k_categorical_accuracy`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6481 samples, validate on 1621 samples\n",
      "Epoch 1/10\n",
      "6481/6481 [==============================] - 1s 223us/step - loss: 3.1098 - sparse_categorical_accuracy: 0.2785 - sparse_top_k_categorical_accuracy: 0.5377 - val_loss: 3.2279 - val_sparse_categorical_accuracy: 0.2782 - val_sparse_top_k_categorical_accuracy: 0.4972\n",
      "Epoch 2/10\n",
      "6481/6481 [==============================] - 1s 139us/step - loss: 3.0914 - sparse_categorical_accuracy: 0.2814 - sparse_top_k_categorical_accuracy: 0.5427 - val_loss: 3.2194 - val_sparse_categorical_accuracy: 0.2788 - val_sparse_top_k_categorical_accuracy: 0.4911\n",
      "Epoch 3/10\n",
      "6481/6481 [==============================] - 1s 139us/step - loss: 3.0608 - sparse_categorical_accuracy: 0.2850 - sparse_top_k_categorical_accuracy: 0.5451 - val_loss: 3.2125 - val_sparse_categorical_accuracy: 0.2788 - val_sparse_top_k_categorical_accuracy: 0.4935\n",
      "Epoch 4/10\n",
      "6481/6481 [==============================] - 1s 145us/step - loss: 3.0397 - sparse_categorical_accuracy: 0.2922 - sparse_top_k_categorical_accuracy: 0.5559 - val_loss: 3.2086 - val_sparse_categorical_accuracy: 0.2782 - val_sparse_top_k_categorical_accuracy: 0.4985\n",
      "Epoch 5/10\n",
      "6481/6481 [==============================] - 1s 153us/step - loss: 3.0315 - sparse_categorical_accuracy: 0.2864 - sparse_top_k_categorical_accuracy: 0.5527 - val_loss: 3.2032 - val_sparse_categorical_accuracy: 0.2795 - val_sparse_top_k_categorical_accuracy: 0.5015\n",
      "Epoch 6/10\n",
      "6481/6481 [==============================] - 1s 165us/step - loss: 3.0032 - sparse_categorical_accuracy: 0.2893 - sparse_top_k_categorical_accuracy: 0.5547 - val_loss: 3.1988 - val_sparse_categorical_accuracy: 0.2788 - val_sparse_top_k_categorical_accuracy: 0.5022\n",
      "Epoch 7/10\n",
      "6481/6481 [==============================] - 1s 155us/step - loss: 2.9948 - sparse_categorical_accuracy: 0.2936 - sparse_top_k_categorical_accuracy: 0.5590 - val_loss: 3.1980 - val_sparse_categorical_accuracy: 0.2801 - val_sparse_top_k_categorical_accuracy: 0.5022\n",
      "Epoch 8/10\n",
      "6481/6481 [==============================] - 1s 154us/step - loss: 2.9837 - sparse_categorical_accuracy: 0.2915 - sparse_top_k_categorical_accuracy: 0.5640 - val_loss: 3.1973 - val_sparse_categorical_accuracy: 0.2813 - val_sparse_top_k_categorical_accuracy: 0.5003\n",
      "Epoch 9/10\n",
      "6481/6481 [==============================] - 1s 163us/step - loss: 2.9545 - sparse_categorical_accuracy: 0.2929 - sparse_top_k_categorical_accuracy: 0.5675 - val_loss: 3.1968 - val_sparse_categorical_accuracy: 0.2801 - val_sparse_top_k_categorical_accuracy: 0.4972\n",
      "Epoch 10/10\n",
      "6481/6481 [==============================] - 1s 147us/step - loss: 2.9505 - sparse_categorical_accuracy: 0.2972 - sparse_top_k_categorical_accuracy: 0.5674 - val_loss: 3.1951 - val_sparse_categorical_accuracy: 0.2813 - val_sparse_top_k_categorical_accuracy: 0.4985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f743828>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = basket_data.dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance of this model to a very naive baseline that just always predicts the most popular product (the goal is obviously to beat it ^^ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_product, max_count = basket_data.counter.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_labels = np.zeros_like(y_test, dtype=np.float32)\n",
    "naive_labels = most_common_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23442319555829735"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(naive_labels == y_test).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
