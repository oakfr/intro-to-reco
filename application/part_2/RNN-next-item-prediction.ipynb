{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for next product prediction\n",
    "\n",
    "The goal of this notebook is to look at a simple implementation of RNN for next product prediction.\n",
    "It might seem as an obvious use case, but it wasn't very popular until the 2015 paper [Session-based Recommendations with Recurrent Neural Networks](https://arxiv.org/abs/1511.06939).\n",
    "\n",
    "We will be using a sample of the [RecSys 2015 challenge yoochoose dataset](https://2015.recsyschallenge.com/challenge.html), where we will user click sessions of various lengths, and the task we will be trying to solve, is predicting at each timestep the next product the user will click."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import sys\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to load the data from a url. We will several sessions, where a session is a succession of clicks on products, basically a shopping session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_url(path):\n",
    "    sessions_df = pd.read_csv(path, sep=\",\", header=None)\n",
    "    sessions_df.columns = [\"session_id\", \"timestamp\", \"item_id\", \"category\"]\n",
    "    sessions_df[\"timestamp\"] = pd.to_datetime(sessions_df[\"timestamp\"])\n",
    "    return sessions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://www.dropbox.com/s/urf0v28umc7afg2/yoochoose-clicks-sample.dat?dl=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df = load_df_from_url(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07 10:51:09.277000+00:00</td>\n",
       "      <td>214536502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07 10:54:09.868000+00:00</td>\n",
       "      <td>214536500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07 10:54:46.998000+00:00</td>\n",
       "      <td>214536506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-07 10:57:00.306000+00:00</td>\n",
       "      <td>214577561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-07 13:56:37.614000+00:00</td>\n",
       "      <td>214662742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id                        timestamp    item_id  category\n",
       "0           1 2014-04-07 10:51:09.277000+00:00  214536502         0\n",
       "1           1 2014-04-07 10:54:09.868000+00:00  214536500         0\n",
       "2           1 2014-04-07 10:54:46.998000+00:00  214536506         0\n",
       "3           1 2014-04-07 10:57:00.306000+00:00  214577561         0\n",
       "4           2 2014-04-07 13:56:37.614000+00:00  214662742         0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the actual sessions which just a list of products. We will group rows by session_id, replace product ids with their indices (between 1 and product_vocab_size, we will see why we are 1-indexed later), and filter out products that are not very common and sessions that are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sessions(sessions_df: pd.DataFrame, \n",
    "                   max_products: int = 1000, \n",
    "                   min_session_size: int = 3) -> List[List[int]]:\n",
    "    print(\"Session Dataframe length \", len(sessions_df))\n",
    "    \n",
    "    all_items = sessions_df[\"item_id\"].values\n",
    "    items_counter = collections.Counter(all_items)\n",
    "    most_common_items = dict(items_counter.most_common(max_products))\n",
    "    ids_to_indices = dict((item_id, i+1) for i, item_id in enumerate(most_common_items.keys()))\n",
    "    \n",
    "    session_dicts = sessions_df.to_dict(orient='records')\n",
    "    grouped_sessions = itertools.groupby(session_dicts, lambda d: d[\"session_id\"])\n",
    "    sessions = []\n",
    "    for _, session in grouped_sessions:\n",
    "        item_list = [d[\"item_id\"] for d in sorted(list(session), key=lambda x: x[\"timestamp\"])]\n",
    "        item_list = [ids_to_indices[item] for item in item_list if item in ids_to_indices]\n",
    "        if len(item_list) >= min_session_size:\n",
    "            sessions.append(item_list)\n",
    "    \n",
    "    print(\"Sessions count \", len(sessions))\n",
    "    \n",
    "    return sessions, most_common_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Dataframe length  5000000\n",
      "Sessions count  447161\n"
     ]
    }
   ],
   "source": [
    "sessions, most_common_items = build_sessions(sessions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't have sessions of different lengths in the same tensor (or numpy array). So we need to fix a session length and truncate sessions to this length, or pad them with a dummy value, we will use 0 as our dummy value, and that is why we adopted 1-based indexing previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_session_length = 50\n",
    "padded_sessions = pad_sequences(sessions, \n",
    "                                maxlen=max_session_length, \n",
    "                                padding='post', \n",
    "                                truncating='pre', \n",
    "                                value=0)\n",
    "padded_sessions = np.array(padded_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a very simple LSTM model, where we will first embed the products in a lower dimensional space and then use the output of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_11 (Masking)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 49, 20)            20020     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 49, 100)           48400     \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 49, 1001)          101101    \n",
      "=================================================================\n",
      "Total params: 169,521\n",
      "Trainable params: 169,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(most_common_items) + 1\n",
    "embedding_size = 20\n",
    "input_length = max_session_length - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(input_length, )))\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=input_length, mask_zero=True))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy_sequential(y_true, y_pred):\n",
    "    y_true = tf.squeeze(y_true)\n",
    "    padding_mask = tf.greater(y_true, 0)\n",
    "    \n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    match = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
    "\n",
    "    match_masked = match * tf.cast(padding_mask, tf.float32)\n",
    "    return tf.reduce_sum(match_masked) / tf.reduce_sum(tf.cast(padding_mask, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_sessions[:, :-1]\n",
    "y = np.expand_dims(padded_sessions[:, 1:], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 402444 samples, validate on 44717 samples\n",
      "Epoch 1/1\n",
      "402444/402444 [==============================] - 2004s 5ms/step - loss: 3.7852 - categorical_accuracy_sequential: 0.1067 - val_loss: 3.1787 - val_categorical_accuracy_sequential: 0.1656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1288c2128>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[categorical_accuracy_sequential])\n",
    "model.fit(x=X, y=y, validation_split=0.1, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
